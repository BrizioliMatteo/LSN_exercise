{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "By keeping fixed all the other parameters, try to use at least two other optimizers, different from SGD. <span style=\"color:red\">Watch to accuracy and loss for training and validation data and comment on the performances</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "Y_train shape: (60000,)\n",
      "\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "... and with label [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] after to_categorical\n",
      "\n",
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n",
      "Model architecture created successfully!\n",
      "Model compiled successfully and ready to be trained.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.2737 - acc: 0.9200 - val_loss: 0.1115 - val_acc: 0.9647\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.1198 - acc: 0.9655 - val_loss: 0.0955 - val_acc: 0.9719\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0864 - acc: 0.9745 - val_loss: 0.0797 - val_acc: 0.9769\n",
      "Epoch 4/5\n",
      "22656/60000 [==========>...................] - ETA: 5s - loss: 0.0621 - acc: 0.9808"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "# output\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# reshape data, it could depend on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "\n",
    "# cast floats to single precesion\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices, e.g. for use with categorical_crossentropy\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "print('... and with label', Y_train[20], 'after to_categorical')\n",
    "print()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "def create_DNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('Model architecture created successfully!')\n",
    "\n",
    "\n",
    "def compile_model():\n",
    "    # create the model\n",
    "    model=create_DNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model compiled successfully and ready to be trained.')\n",
    "\n",
    "\n",
    "#training parameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# create the deep neural net\n",
    "model_DNN = compile_model()\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history = model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_DNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best');plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best');plt.show()\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "predictions = model_DNN.predict(X_test)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15)) \n",
    "for i in range(10):    \n",
    "    ax = plt.subplot(2, 10, i + 1)    \n",
    "    plt.imshow(X_test[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted: {}\".format(np.argmax(Y_test[i]), np.argmax(predictions[i])))    \n",
    "    plt.axis('off');\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando diversi *optimizer*, ho notato che quello che abbassava di pi√π il model loss e la cui accuratezza era maggiore era l'*Adam*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2\n",
    "\n",
    "Change the architecture of your DNN using convolutional layers. Use `Conv2D`, `MaxPooling2D`, `Dropout`, but also do not forget `Flatten`, a standard `Dense` layer and `soft-max` in the end. I have merged step 2 and 3 in the following definition of `create_CNN()` that **<span style=\"color:red\">you should complete</span>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need the following for Convolutional Neural Networks\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add first convolutional layer with 10 filters (dimensionality of output space)\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(30, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))   \n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    #\n",
    "    # ADD HERE SOME OTHER LAYERS AT YOUR WILL, FOR EXAMPLE SOME: Dropout, 2D pooling, 2D convolutional etc. ... \n",
    "    # remember to move towards a standard flat layer in the final part of your DNN,\n",
    "    # and that we need a soft-max layer with num_classes=10 possible outputs\n",
    "    #\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rete √® costituira da un primo convolution layer, questo perch√® ci aspettiamo che i pixels siano legati con quelli vicini e la convoluzione permette di preservare le relazioni tra le diverse parti dell'immagine riducendo per√≤ la complessit√†. A qui abbiamo aggiunto un pooling layer che permette di ridurre la dimensionalit√† e quindi il numero di parametri contro i problemi di overfitting. Si √® ripetuta due volte tale operazione scegliendo come activation per i Convolution la funzione **Relu**. A questo punto si √® utilizzato *Flatten* e creato un fully connected layer inserendo inoltre un funzione *Dropout* che inibisce alcuni neuroni in modo da aiutare il problema dell'overfitting. L'ultimo layer utilizza una funzione di attivazione di tipo soft-max che per ogni neurone esegue una sorta di normalizzazione producendo come una probabilit√† con valori compresi tra $0$ e $1$.\n",
    "\n",
    "\n",
    "Con le operazioni inserite prima del Flatten, la matrice ha subito le seguenti modifiche: $28 \\times 28\\rightarrow 24 \\times 24 \\times 10 \\rightarrow 12 \\times 12 \\times 10 \\rightarrow 8 \\times 8 \\times 30 \\rightarrow 4\\times 4 \\times 30  $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# create the deep conv net\n",
    "model_CNN=create_CNN()\n",
    "\n",
    "# train CNN\n",
    "history=model_CNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best');plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best');plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "\n",
    "predictions = model_CNN.predict(X_test)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "plt.figure(figsize=(15, 15)) \n",
    "for i in range(10):    \n",
    "    ax = plt.subplot(2, 10, i + 1)    \n",
    "    plt.imshow(X_test[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted:{}\".format(np.argmax(Y_test[i]), np.argmax(predictions[i])))    \n",
    "    plt.axis('off');\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "Use the `gimp` application to create 10 pictures of your \"handwritten\" digits, import them in your jupyter-notebook and try to see if your CNN is able to recognize your handwritten digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "data = np.zeros((10,28, 28))\n",
    "plt.figure(figsize=(15, 15)) \n",
    "\n",
    "for k in range (10):\n",
    "    digit_filename = \"Digits/\"+str(k)+\".png\"\n",
    "    digit_in = Image.open(digit_filename).convert('L')\n",
    "    ydim, xdim = digit_in.size\n",
    "    pix=digit_in.load();\n",
    "    for j in range(ydim):\n",
    "        for i in range(xdim):\n",
    "            data[k,i,j]=pix[j,i]\n",
    "    \n",
    "    data[k,:,:] /= 255\n",
    "\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    data = data.reshape(data.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    data = data.reshape(data.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "pred_0 = model_CNN.predict(data)\n",
    "\n",
    "data = data.reshape(data.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "plt.figure(figsize=(15, 15)) \n",
    "for i in range(10):    \n",
    "    ax = plt.subplot(2, 10, i + 1)    \n",
    "    plt.imshow(data[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted:{}\".format(i, np.argmax(pred_0[i])))    \n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rete neurale riesce a riconoscere i numeri da me scritti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
